{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a60549-30f9-4e16-85c4-d33069540378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intel\\Desktop\\dektoop\\amhcd-data-64/tifinagh images/\n",
      "C:\\Users\\intel\\Desktop\\dektoop\n",
      "labels-map.csv not found. Please check the dataset structure.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'C:\\\\Users\\\\intel\\\\Desktop\\\\dektoop\\\\amhcd-data-64/tifinagh images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 204\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     labels_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamhcd-data-64/labels-map.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels_df\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\opencv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\opencv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\opencv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\opencv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\opencv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m         handle,\n\u001b[0;32m    875\u001b[0m         ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m         newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m     )\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\intel\\\\Desktop\\\\dektoop\\\\amhcd-data-64/tifinagh images/amhcd-data-64/labels-map.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 211\u001b[0m\n\u001b[0;32m    209\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    210\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label_dir \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_dir):\n\u001b[0;32m    212\u001b[0m     label_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, label_dir)\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(label_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'C:\\\\Users\\\\intel\\\\Desktop\\\\dektoop\\\\amhcd-data-64/tifinagh images/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Fonctions d’activation\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation: max(0, x)\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU must be a numpy array\"\n",
    "    result = np.maximum(0, x)\n",
    "    assert np.all(result >= 0), \"ReLU output must be non-negative\"\n",
    "    return result\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU: 1 if x > 0, else 0\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU derivative must be a numpy array\"\n",
    "    result = np.where(x > 0, 1, 0)\n",
    "    assert np.all((result == 0) | (result == 1)), \"ReLU derivative must be 0 or 1\"\n",
    "    return result\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax activation: exp(x) / sum(exp(x))\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to softmax must be a numpy array\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    result = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    assert np.all((result >= 0) & (result <= 1)), \"Softmax output must be in [0, 1]\"\n",
    "    assert np.allclose(np.sum(result, axis=1), 1), \"Softmax output must sum to 1 per sample\"\n",
    "    return result\n",
    "\n",
    "# Classe MultiClassNeuralNetwork\n",
    "class MultiClassNeuralNetwork:\n",
    "    def _init__(self, layer_sizes, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with given layer sizes and learning rate.\n",
    "        layer_sizes: List of integers [input_size, hidden1_size, ..., output_size]\n",
    "        \"\"\"\n",
    "        assert isinstance(layer_sizes, list) and len(layer_sizes) >= 2, \"layer_sizes must be a list with at least 2 elements\"\n",
    "        assert all(isinstance(size, int) and size > 0 for size in layer_sizes), \"All layer sizes must be positive integers\"\n",
    "        assert isinstance(learning_rate, (int, float)) and learning_rate > 0, \"Learning rate must be a positive number\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_lambda = 0.01  # Added for L2 regularization\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        # Initialisation des poids et biais\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])  # He initialization\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            assert w.shape == (layer_sizes[i], layer_sizes[i+1]), f\"Weight matrix {i+1} has incorrect shape\"\n",
    "            assert b.shape == (1, layer_sizes[i+1]), f\"Bias vector {i+1} has incorrect shape\"\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation: Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}, A^{[l]} = g(Z^{[l]})\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            assert z.shape == (X.shape[0], self.layer_sizes[i+1]), f\"Z^{[i+1]} has incorrect shape\"\n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(relu(z))\n",
    "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        assert z.shape == (X.shape[0], self.layer_sizes[-1]), \"Output Z has incorrect shape\"\n",
    "        self.z_values.append(z)\n",
    "        output = softmax(z)\n",
    "        assert output.shape == (X.shape[0], self.layer_sizes[-1]), \"Output A has incorrect shape\"\n",
    "        self.activations.append(output)\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Categorical Cross-Entropy: J = -1/m * sum(y_true * log(y_pred)) + L2 regularization\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to loss must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        m = y_true.shape[0]\n",
    "        cross_entropy = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        l2_loss = self.l2_lambda * sum(np.sum(np.square(w)) for w in self.weights) / (2 * m)\n",
    "        loss = cross_entropy + l2_loss\n",
    "        assert not np.isnan(loss), \"Loss computation resulted in NaN\"\n",
    "        return loss\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute accuracy: proportion of correct predictions\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to accuracy must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \" [y_true and y_pred must have the same shape\"\n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        assert 0 <= accuracy <= 1, \"Accuracy must be between 0 and 1\"\n",
    "        return accuracy\n",
    "\n",
    "    def backward(self, X, y, outputs):\n",
    "        \"\"\"\n",
    "        Backpropagation: compute dW^{[l]}, db^{[l]} for each layer\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray) and isinstance(outputs, np.ndarray), \"Inputs to backward must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape == outputs.shape, \"y and outputs must have the same shape\"\n",
    "        m = X.shape[0]\n",
    "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Gradient for output layer (softmax + cross-entropy)\n",
    "        dZ = outputs - y\n",
    "        assert dZ.shape == outputs.shape, \"dZ for output layer has incorrect shape\"\n",
    "        self.d_weights[-1] = (np.dot(self.activations[-2].T, dZ) + self.l2_lambda * self.weights[-1]) / m\n",
    "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Backpropagation for hidden layers\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            dZ = np.dot(dZ, self.weights[i+1].T) * relu_derivative(self.z_values[i])\n",
    "            assert dZ.shape == (X.shape[0], self.layer_sizes[i+1]), f\"dZ^{[i+1]} has incorrect shape\"\n",
    "            self.d_weights[i] = (np.dot(self.activations[i].T, dZ) + self.l2_lambda * self.weights[i]) / m\n",
    "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update parameters\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
    "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch SGD, with validation\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n",
    "        assert isinstance(X_val, np.ndarray) and isinstance(y_val, np.ndarray), \"X_val and y_val must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape[1] == self.layer_sizes[-1], f\"Output dimension ({y.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert X_val.shape[1] == self.layer_sizes[0], f\"Validation input dimension ({X_val.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y_val.shape[1] == self.layer_sizes[-1], f\"Validation output dimension ({y_val.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert isinstance(epochs, int) and epochs > 0, \"Epochs must be a positive integer\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \"Batch size must be a positive integer\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                outputs = self.forward(X_batch)\n",
    "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
    "                self.backward(X_batch, y_batch, outputs)\n",
    "            # Calculer les pertes et accuracies\n",
    "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
    "            train_pred = self.forward(X)\n",
    "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
    "            val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, val_pred)\n",
    "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        outputs = self.forward(X)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        assert predictions.shape == (X.shape[0],), \"Predictions have incorrect shape\"\n",
    "        return predictions\n",
    "\n",
    "# Définir le chemin vers le dossier décompressé\n",
    "data_dir = os.path.join(os.getcwd(), 'amhcd-data-64/tifinagh images/')\n",
    "print(data_dir)\n",
    "current_working_directory = os.getcwd()\n",
    "print(current_working_directory)\n",
    "\n",
    "# Charger le fichier CSV contenant les étiquettes\n",
    "try:\n",
    "    labels_df = pd.read_csv(os.path.join(data_dir, 'amhcd-data-64/labels-map.csv'))\n",
    "    assert 'image_path' in labels_df.columns and 'label' in labels_df.columns, \"CSV must contain 'image_path' and 'label' columns\"\n",
    "except FileNotFoundError:\n",
    "    print(\"labels-map.csv not found. Please check the dataset structure.\")\n",
    "    # Alternative : construire un DataFrame à partir des dossiers\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label_dir in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label_dir)\n",
    "        if os.path.isdir(label_path):\n",
    "            for img_name in os.listdir(label_path):\n",
    "                image_paths.append(os.path.join(label_path, img_name))\n",
    "                labels.append(label_dir)\n",
    "    labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
    "\n",
    "# Vérifier le DataFrame\n",
    "assert not labels_df.empty, \"No data loaded. Check dataset files.\"\n",
    "print(f\"Loaded {len(labels_df)} samples with {labels_df['label'].nunique()} unique classes.\")\n",
    "\n",
    "# Encoder les étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Fonction pour charger et prétraiter une image\n",
    "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image: convert to grayscale, resize, normalize\n",
    "    \"\"\"\n",
    "    assert os.path.exists(image_path), f\"Image not found: {image_path}\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    assert img is not None, f\"Failed to load image: {image_path}\"\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype(np.float32) / 255.0  # Normalisation\n",
    "    return img.flatten()  # Aplatir pour le réseau de neurones\n",
    "\n",
    "# Charger toutes les images\n",
    "X = np.array([load_and_preprocess_image(os.path.join(data_dir, path)) for path in labels_df['image_path']])\n",
    "y = labels_df['label_encoded'].values\n",
    "\n",
    "# Vérifier les dimensions\n",
    "assert X.shape[0] == y.shape[0], \"Mismatch between number of images and labels\"\n",
    "assert X.shape[1] == 32 * 32, f\"Expected flattened image size of {32*32}, got {X.shape[1]}\"\n",
    "\n",
    "# Diviser en ensembles d’entraînement, validation et test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Convertir explicitement en NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "assert X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == X.shape[0], \"Train-val-test split sizes must sum to total samples\"\n",
    "print(f\"Train: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Encoder les étiquettes en one-hot pour la classification multiclasse\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_one_hot = np.array(one_hot_encoder.fit_transform(y_train.reshape(-1, 1)))\n",
    "y_val_one_hot = np.array(one_hot_encoder.transform(y_val.reshape(-1, 1)))\n",
    "y_test_one_hot = np.array(one_hot_encoder.transform(y_test.reshape(-1, 1)))\n",
    "\n",
    "# Vérifier que les tableaux one-hot sont des NumPy arrays\n",
    "assert isinstance(y_train_one_hot, np.ndarray), \"y_train_one_hot must be a numpy array\"\n",
    "assert isinstance(y_val_one_hot, np.ndarray), \"y_val_one_hot must be a numpy array\"\n",
    "assert isinstance(y_test_one_hot, np.ndarray), \"y_test_one_hot must be a numpy array\"\n",
    "\n",
    "# Créer et entraîner le modèle vowels\n",
    "layer_sizes = [X_train.shape[1], 64, 32, num_classes]  # 64 et 32 neurones cachés, 33 classes\n",
    "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.01)\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
    "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32\n",
    ")\n",
    "\n",
    "# TODO: Ajouter une validation croisée pour évaluer la robustesse du modèle\n",
    "# TODO: Implémenter l’optimiseur Adam pour une meilleure convergence\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matrice de confusion (Test set)')\n",
    "plt.xlabel('Prédit')\n",
    "plt.ylabel('Réel')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Courbes de perte et d’accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Courbe de perte\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_title('Courbe de perte')\n",
    "ax1.set_xlabel('Époque')\n",
    "ax1.set_ylabel('Perte')\n",
    "ax1.legend()\n",
    "\n",
    "# Courbe d’accuracy\n",
    "ax2.plot(train_accuracies, label='Train Accuracy')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "ax2.set_title('Courbe de précision')\n",
    "ax2.set_xlabel('Époque')\n",
    "ax2.set_ylabel('Précision')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('loss_accuracy_plot.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
